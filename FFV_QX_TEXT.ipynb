{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582c0128-ae74-4b37-8729-a8dd58a8cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c313174-be79-4efc-a1bb-5e867149be73",
   "metadata": {},
   "source": [
    "## Find the Important Features to Predict `Nutrition Density`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fec8d8ed-95dc-4484-abd9-be1d9b2b6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"clean_data_with_same_units.csv\", index_col=0)\n",
    "\n",
    "# Separate predictors and target\n",
    "X = df.drop(columns=['Nutrition Density', 'food', 'Caloric Value'])\n",
    "y = df['Nutrition Density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10b1a59f-2d5f-42f4-a246-f4c30df019c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (only on training data)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit ElasticNet only on training data\n",
    "best_en = ElasticNet(alpha=0.01, l1_ratio=0.9, max_iter=5000)\n",
    "best_en.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Select important features based on training data\n",
    "selected_features = X_train.columns[best_en.coef_ != 0]\n",
    "\n",
    "# Filter dataset using selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Define best hyperparameters manually\n",
    "best_rf = RandomForestRegressor(\n",
    "    max_depth=13,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=10,\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the Random Forest model with the best hyperparameters\n",
    "best_rf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_rf.predict(X_test_selected)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Adjusted R-squared calculation\n",
    "n = X_test_selected.shape[0]  # number of samples\n",
    "p = X_test_selected.shape[1]  # number of predictors\n",
    "adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1))\n",
    "\n",
    "# Corrected AIC calculation\n",
    "rss = mse * n  # Residual Sum of Squares\n",
    "aic = n * np.log(rss / n) + 2 * (p + 1)\n",
    "\n",
    "results = {\n",
    "    \"Test MSE\": mse,\n",
    "    \"Test RMSE\": rmse,\n",
    "    \"Test R2\": r2,\n",
    "    \"Test Adjusted R2\": adj_r2,\n",
    "    \"Test AIC\": aic\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "637be50b-5b2b-4725-883f-d596f7e75126",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Training Set Evaluation**\n",
    "y_train_pred = best_rf.predict(X_train_selected)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Adjusted R-squared for training set\n",
    "n_train = X_train_selected.shape[0]  # number of training samples\n",
    "p_train = X_train_selected.shape[1]  # number of predictors\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# AIC for training set\n",
    "aic_train = n_train * np.log(mse_train) + 2 * (p_train + 1)\n",
    "\n",
    "# Compute Cross-Validation Error\n",
    "cv_mse = -cross_val_score(best_rf, X_train_selected, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse = np.sqrt(cv_mse)\n",
    "cv_r2 = cross_val_score(best_rf, X_train_selected, y_train, cv=5, scoring='r2')\n",
    "\n",
    "# Adjusted R2 for cross-validation\n",
    "adj_r2_cv = 1 - ((1 - np.mean(cv_r2)) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# AIC for cross-validation\n",
    "aic_cv = n_train * np.log(np.mean(cv_mse)) + 2 * (p_train + 1)\n",
    "\n",
    "train_results = {\n",
    "    \"Train MSE\": mse_train,\n",
    "    \"Train RMSE\": rmse_train,\n",
    "    \"Train R2\": r2_train,\n",
    "    \"Train Adjusted R2\": adj_r2_train,\n",
    "    \"Train AIC\": aic_train,\n",
    "    \"Cross-Validation MSE\": np.mean(cv_mse),\n",
    "    \"Cross-Validation RMSE\": np.mean(cv_rmse),\n",
    "    \"Cross-Validation R2\": np.mean(cv_r2),\n",
    "    \"Cross-Validation Adjusted R2\": adj_r2_cv,\n",
    "    \"Cross-Validation AIC\": aic_cv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3476d566-6bfc-440c-b311-55554153b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_importances = best_rf.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': selected_features, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "nutrition_columns = selected_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eeeb78e1-13eb-4e96-96de-8f5c805736c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fat',\n",
       " 'Saturated Fats',\n",
       " 'Monounsaturated Fats',\n",
       " 'Polyunsaturated Fats',\n",
       " 'Carbohydrates',\n",
       " 'Sugars',\n",
       " 'Protein',\n",
       " 'Dietary Fiber',\n",
       " 'Cholesterol',\n",
       " 'Sodium',\n",
       " 'Water',\n",
       " 'Vitamin A',\n",
       " 'Vitamin B3',\n",
       " 'Vitamin B5',\n",
       " 'Vitamin C',\n",
       " 'Vitamin D',\n",
       " 'Calcium',\n",
       " 'Iron',\n",
       " 'Manganese',\n",
       " 'Phosphorus',\n",
       " 'Potassium',\n",
       " 'Selenium',\n",
       " 'Zinc']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nutrition_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ef2737e-0723-4b4f-8610-c8bc6175d060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# Save this list for later use \n",
    "with open(\"nutrition_columns.json\", \"w\") as f:\n",
    "    json.dump(nutrition_columns, f)\n",
    "\n",
    "\n",
    "# Save ElasticNet model for feature selection\n",
    "joblib.dump(best_en, \"elastic_net_model.pkl\")\n",
    "\n",
    "# Save RandomForest model for final predictions\n",
    "joblib.dump(best_rf, \"random_forest_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369dcc0-957e-4dc4-9a10-df0cba2db434",
   "metadata": {},
   "source": [
    "## NLP - Extract Food Name Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5186cb51-325a-474f-93a1-efc26dab2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "path = \"clean_data_with_same_units.csv\"\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "# Store a copy of the original numeric nutrition data\n",
    "df_original = df.copy()\n",
    "\n",
    "# Load spaCy NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def parse_food_name(food_name):\n",
    "    \"\"\"Extract meaningful components from food name using NLP.\"\"\"\n",
    "    doc = nlp(food_name.lower())  # Convert to lowercase\n",
    "    return [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]  \n",
    "    # Lemmatization, remove stop-words & non-alphabetic tokens\n",
    "\n",
    "# Apply parsing\n",
    "df[\"parsed_food\"] = df[\"food\"].apply(parse_food_name)\n",
    "\n",
    "# Extract unique components using itertools.chain for better performance\n",
    "all_components = list(set(chain.from_iterable(df[\"parsed_food\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cbccb09-211e-421f-8f9d-dfa9e6de8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"food_name_components.json\", \"w\") as f:\n",
    "    json.dump(all_components, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2550a27-571c-427e-aecb-7e812409edd1",
   "metadata": {},
   "source": [
    "## Create a Binary Matrix for Food Components - Using least squares regression to estimate the contribution of each food component to different nutrients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2120ecf-a52d-4c09-9411-1ecce2c52519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty DataFrame for one-hot encoding (food components)\n",
    "food_component_df = pd.DataFrame(0, index=df.index, columns=all_components, dtype=int)\n",
    "\n",
    "# Populate the binary matrix using parsed food names\n",
    "for i, components in enumerate(df[\"parsed_food\"]):\n",
    "    for component in set(components):  # Avoid duplicate entries\n",
    "        if component in food_component_df.columns:\n",
    "            food_component_df.iat[i, food_component_df.columns.get_loc(component)] = 1\n",
    "\n",
    "# Merge one-hot encoded matrix back with the original DataFrame\n",
    "df = pd.concat([df, food_component_df], axis=1)\n",
    "\n",
    "# Get numeric nutrition columns from the original dataset (excluding target variable)\n",
    "nutrition_columns = df_original.select_dtypes(include=[np.number]).columns.drop(\"Nutrition Density\")\n",
    "\n",
    "# Convert numeric columns to float and fill NaNs with 0\n",
    "df_original[nutrition_columns] = df_original[nutrition_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Prepare matrices\n",
    "X = food_component_df.values  # One-hot encoded matrix (food × components)\n",
    "Y = df_original[nutrition_columns].values.astype(np.float64)  # Nutrient matrix (food × nutrients)\n",
    "\n",
    "# Solve least squares problem to estimate nutrient contributions\n",
    "W, residuals, rank, s = np.linalg.lstsq(X, Y, rcond=None)\n",
    "\n",
    "# Convert results into a DataFrame (components × nutrients)\n",
    "component_nutrition_contributions = pd.DataFrame(W, index=food_component_df.columns, columns=nutrition_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4590218c-e3f0-4937-94a6-79b694b429b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for later use in the app\n",
    "component_nutrition_contributions.to_csv(\"component_nutrition_contributions.csv\")\n",
    "component_nutrition_contributions.to_json(\"component_nutrition_contributions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b4371-bbab-41e8-b5f7-31ee64a2be2e",
   "metadata": {},
   "source": [
    "## NLP - Use word embeddings - Match new/unknown food names to the most similar ones in existing food dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14b2a0a4-9207-440a-adb4-e39683fdf6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load medium-sized spaCy model (better than en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Load food components from pickle\n",
    "with open(\"food_components.pkl\", \"rb\") as f:\n",
    "    all_components = pickle.load(f)\n",
    "\n",
    "# Load nutrition data from CSV\n",
    "component_nutrition_contributions = pd.read_csv(\"component_nutrition_contributions.csv\", index_col=0)\n",
    "\n",
    "# Store precomputed word vectors for all known components (avoid recomputation)\n",
    "component_vectors = {}\n",
    "for word in all_components:\n",
    "    word_vector = nlp(word).vector\n",
    "    if np.any(word_vector):  # Ensure vector exists (some words may be OOV)\n",
    "        component_vectors[word] = word_vector\n",
    "\n",
    "# Save precomputed vectors for fast future use\n",
    "with open(\"component_vectors.pkl\", \"wb\") as f:\n",
    "    pickle.dump(component_vectors, f)\n",
    "\n",
    "\n",
    "def find_closest_component(user_input):\n",
    "    \"\"\"Find the closest matching food component using NLP embeddings.\"\"\"\n",
    "    user_words = parse_food_name(user_input)  # Extract words from input\n",
    "    matched_words = []\n",
    "\n",
    "    for word in user_words:\n",
    "        # **Step 1: Check for an exact match**\n",
    "        if word in all_components:\n",
    "            matched_words.append(word)\n",
    "            continue  # Skip similarity check if exact match found\n",
    "\n",
    "        # **Step 2: Compute similarity with known components**\n",
    "        word_vector = nlp(word).vector\n",
    "        if not np.any(word_vector):  # Skip words without a vector\n",
    "            continue\n",
    "\n",
    "        word_vector = word_vector.reshape(1, -1)\n",
    "\n",
    "        similarities = {\n",
    "            comp: cosine_similarity(word_vector, comp_vec.reshape(1, -1))[0][0]\n",
    "            for comp, comp_vec in component_vectors.items()\n",
    "        }\n",
    "\n",
    "        # Get best match\n",
    "        best_match = max(similarities, key=similarities.get)\n",
    "        best_match_score = similarities[best_match]\n",
    "\n",
    "        # **Avoid poor matches** (threshold > 0.5)\n",
    "        if best_match_score > 0.5:\n",
    "            matched_words.append(best_match)\n",
    "\n",
    "    return matched_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49169524-616e-4b3d-9244-b983cf3bce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Components: ['pork', 'rice', 'flour']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_input = \"bbq rice flour\"\n",
    "matched_components = find_closest_component(user_input)\n",
    "print(f\"Matched Components: {matched_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65316494-92bf-43b7-be35-211a752884ac",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cbbc015d-edaf-449a-b262-5b8c17d43170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food_matching.py saved successfully!\n"
     ]
    }
   ],
   "source": [
    "script = \"\"\"import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load medium-sized spaCy model (better than en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Load food components from pickle\n",
    "with open(\"food_components.pkl\", \"rb\") as f:\n",
    "    all_components = pickle.load(f)\n",
    "\n",
    "# Load nutrition data\n",
    "component_nutrition_contributions = pd.read_csv(\"component_nutrition_contributions.csv\", index_col=0)\n",
    "\n",
    "# Precompute word vectors for all known food components\n",
    "component_vectors = {}\n",
    "for word in all_components:\n",
    "    word_vector = nlp(word).vector\n",
    "    if np.any(word_vector):  # Ensure word has a vector (avoid OOV issues)\n",
    "        component_vectors[word] = word_vector\n",
    "\n",
    "# Save precomputed vectors for fast reuse\n",
    "with open(\"component_vectors.pkl\", \"wb\") as f:\n",
    "    pickle.dump(component_vectors, f)\n",
    "\n",
    "def parse_food_name(food_name):\n",
    "    '''Extract meaningful words from food names.'''\n",
    "    doc = nlp(food_name.lower())\n",
    "    return [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "\n",
    "def find_closest_component(user_input):\n",
    "    '''Find the closest matching food component using NLP embeddings.'''\n",
    "    user_words = parse_food_name(user_input)  \n",
    "    matched_words = []\n",
    "\n",
    "    for word in user_words:\n",
    "        # Step 1: Check for an exact match\n",
    "        if word in all_components:\n",
    "            matched_words.append(word)\n",
    "            continue  # Skip similarity check if exact match found\n",
    "\n",
    "        # Step 2: Compute similarity with known components\n",
    "        word_vector = nlp(word).vector\n",
    "        if not np.any(word_vector):  # Skip words without a vector\n",
    "            continue\n",
    "\n",
    "        word_vector = word_vector.reshape(1, -1)\n",
    "\n",
    "        similarities = {\n",
    "            comp: cosine_similarity(word_vector, comp_vec.reshape(1, -1))[0][0]\n",
    "            for comp, comp_vec in component_vectors.items()\n",
    "        }\n",
    "\n",
    "        # Get best match\n",
    "        best_match = max(similarities, key=similarities.get)\n",
    "        best_match_score = similarities[best_match]\n",
    "\n",
    "        # Avoid poor matches (threshold > 0.5)\n",
    "        if best_match_score > 0.5:\n",
    "            matched_words.append(best_match)\n",
    "\n",
    "    return matched_words\n",
    "\"\"\"\n",
    "\n",
    "# Save script to a file\n",
    "with open(\"food_matching.py\", \"w\") as f:\n",
    "    f.write(script)\n",
    "\n",
    "print(\"food_matching.py saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1f50229-737c-4b11-ac8b-6f4597f1766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grill', 'chicken', 'breast']\n"
     ]
    }
   ],
   "source": [
    "from food_matching import find_closest_component\n",
    "\n",
    "user_food = \"grilled chicken breast\"\n",
    "matched_components = find_closest_component(user_food)\n",
    "\n",
    "print(matched_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d68e8-c753-4445-8be6-3ddb5c26cb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "115c45c0-139a-41df-b4b8-c1d2b06be99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predicted Nutrition Density saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load trained RandomForest model\n",
    "best_rf = joblib.load(\"random_forest_model.pkl\")\n",
    "\n",
    "# Load selected feature names (used during training)\n",
    "with open(\"nutrition_columns.json\", \"r\") as f:\n",
    "    nutrition_columns = json.load(f)\n",
    "\n",
    "# Load food component nutrient contributions\n",
    "component_nutrition_contributions = pd.read_csv(\"component_nutrition_contributions.csv\", index_col=0)\n",
    "\n",
    "# Ensure we only use features present in both `nutrition_columns` and the dataset\n",
    "nutrition_columns = [col for col in nutrition_columns if col in component_nutrition_contributions.columns]\n",
    "\n",
    "# Select only the columns that were used in training\n",
    "X_components = component_nutrition_contributions[nutrition_columns]\n",
    "\n",
    "# Ensure the feature order matches what was used in training\n",
    "expected_features = list(best_rf.feature_names_in_)  # Features seen during training\n",
    "\n",
    "# Find the intersection between expected features and current dataset\n",
    "matching_features = [col for col in expected_features if col in X_components.columns]\n",
    "\n",
    "# Reorder X_components to match training order\n",
    "X_components = X_components[matching_features]\n",
    "\n",
    "# Check if feature names exactly match before prediction\n",
    "assert list(X_components.columns) == list(best_rf.feature_names_in_), \"Feature names do not match!\"\n",
    "\n",
    "# Predict Nutrition Density\n",
    "predicted_nutrition_density = best_rf.predict(X_components)\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "component_nutrition_contributions[\"Predicted_Nutrition_Density\"] = predicted_nutrition_density\n",
    "\n",
    "# Save predictions\n",
    "component_nutrition_contributions.to_csv(\"component_nutrition_density_predictions.csv\")\n",
    "component_nutrition_contributions.to_json(\"component_nutrition_density_predictions.json\")\n",
    "\n",
    "print(\"✅ Predicted Nutrition Density saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f433c083-45ff-4677-97e0-19b711ad2b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b9eee-b347-4e97-b2d9-2f11bd7a8a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867a70c-c8d2-4cb6-b4f5-eb609ec6b9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47967630-3970-41cf-9527-ce0a1cc3a75f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
